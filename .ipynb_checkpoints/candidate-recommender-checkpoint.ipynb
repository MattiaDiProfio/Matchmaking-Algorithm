{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439c29d3",
   "metadata": {},
   "source": [
    "# SkillPilot - A job recommender system for candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec95282",
   "metadata": {},
   "source": [
    "<a id=\"table-of-contents\"></a>\n",
    " ## Table of Contents  \n",
    "- [Project Overview](#project-overview)     \n",
    "- [Preparing the Data](#preparing-the-data) \n",
    "    - [Candidates data cleaning](#candidates-data-cleaning)\n",
    "    - [Jobs data cleaning](#jobs-data-cleaning)\n",
    "- [Exploratory Data Visualization & Analysis](#exploratory-data-viz) \n",
    "    - [Candidates Data Analysis](#candidates-data-analysis)\n",
    "    - [Jobs Data Analysis](#jobs-data-analysis)\n",
    "- [Gale-Shapley Algorithm](#gale-shapley)\n",
    "    - [Performance Benchmark Decorator](#benchmark-decorator)\n",
    "    - [Configuring Control Variables](#control-variables)\n",
    "    - [Performance Metrics & Evaluation](#performance-accuracy)\n",
    "        - [Jobs vs Iterations Evaluation](#jobs-iterations)\n",
    "        - [Run-time vs Input size](#runtime-inputsize)\n",
    "        - [Percentage of Offers vs Input size](#percentage)\n",
    "    - [Run-time Improvements](#runtime-improvements)\n",
    "        - [Population with Assumption](#compute-matrix-v2)\n",
    "        - [Testing Matrix Population Methods](#testing-population-methods)\n",
    "    - [Improving the percentage of offers computed](#improving-offer-pecentage)\n",
    "    - [Conclusion](#gale-shapley-conclusion)\n",
    "\n",
    "- [Linear Support Vector Machines](#linear-svm)\n",
    "    - [Exploratory Data Analysis](#svm-data-analysis)\n",
    "    - [Proposed Strategy](#svm-strategy)\n",
    "    - [Constructing the Dataframe for SVC](#svm-df-setup)\n",
    "    - [Implementing the SVM classifier](#svm-implementation)\n",
    "    - [Improving the Precision of the SVM classifier](#svm-improvement)\n",
    "    - [Evaluation and Conclusion](#svm-evaluation)\n",
    "\n",
    "- [Project Conclusion](#project-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff62966",
   "metadata": {},
   "source": [
    "<a id=\"project-overview\"></a>\n",
    "## Project Overview\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73e45b",
   "metadata": {},
   "source": [
    "The aim of this project is to come up with a solution capable of accurately matching candidates suitable for a particular job advertisement. I will proceed by conducting some exploratory data analysis on the (randomly generated) datasets, to then proceed with the implementation of common matchmaking algorithms as well as machine learning regression and classification techniques. \n",
    "Finally, I will analyse the performance of each approach to select a suitable option to carry out the pairing procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ae971e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set matplotlib inline styles\n",
    "%matplotlib inline\n",
    "\n",
    "# load the datasets into pandas dataframes\n",
    "jobs = pd.read_csv('jobs.csv')\n",
    "candidates = pd.read_csv('candidates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "76b0139d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Field</th>\n",
       "      <th>MinScore</th>\n",
       "      <th>Positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dental Hygienist Internship</td>\n",
       "      <td>Jimenez, Carney and Foley</td>\n",
       "      <td>physiotherapy</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Registered Nurse Placement</td>\n",
       "      <td>Roth, Rose and Cross</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>81</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dental Hygienist Internship</td>\n",
       "      <td>Russell-Banks</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dental Hygienist Internship</td>\n",
       "      <td>Martin Ltd</td>\n",
       "      <td>immunology</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physician Internship</td>\n",
       "      <td>Lambert Group</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title                    Company          Field  \\\n",
       "0  Dental Hygienist Internship  Jimenez, Carney and Foley  physiotherapy   \n",
       "1   Registered Nurse Placement       Roth, Rose and Cross      nutrition   \n",
       "2  Dental Hygienist Internship              Russell-Banks      nutrition   \n",
       "3  Dental Hygienist Internship                 Martin Ltd     immunology   \n",
       "4         Physician Internship              Lambert Group      nutrition   \n",
       "\n",
       "   MinScore  Positions  \n",
       "0        76          7  \n",
       "1        81          8  \n",
       "2        87         10  \n",
       "3        79          3  \n",
       "4        80          9  "
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkout the head of the jobs dataframe\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "76b291ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fullname</th>\n",
       "      <th>Course</th>\n",
       "      <th>Score</th>\n",
       "      <th>Experience</th>\n",
       "      <th>StudyMode</th>\n",
       "      <th>StudyPattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ashley Harris</td>\n",
       "      <td>Doctor of Pharmacy (Pharm.D.)</td>\n",
       "      <td>96</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria Harris</td>\n",
       "      <td>Master of Health Administration (M.H.A.)</td>\n",
       "      <td>77</td>\n",
       "      <td>surgery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thomas Boyer</td>\n",
       "      <td>Bachelor of Science in Biomedical Science (B.S.)</td>\n",
       "      <td>86</td>\n",
       "      <td>immunology</td>\n",
       "      <td>online</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holly Friedman</td>\n",
       "      <td>Bachelor of Science in Nursing (B.S.N.)</td>\n",
       "      <td>82</td>\n",
       "      <td>physiotherapy</td>\n",
       "      <td>online</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rebecca Cook DVM</td>\n",
       "      <td>Doctor of Pharmacy (Pharm.D.)</td>\n",
       "      <td>63</td>\n",
       "      <td>nursing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fullname                                            Course  Score  \\\n",
       "0     Ashley Harris                     Doctor of Pharmacy (Pharm.D.)     96   \n",
       "1      Maria Harris          Master of Health Administration (M.H.A.)     77   \n",
       "2      Thomas Boyer  Bachelor of Science in Biomedical Science (B.S.)     86   \n",
       "3    Holly Friedman           Bachelor of Science in Nursing (B.S.N.)     82   \n",
       "4  Rebecca Cook DVM                     Doctor of Pharmacy (Pharm.D.)     63   \n",
       "\n",
       "      Experience StudyMode StudyPattern  \n",
       "0      nutrition       NaN           FT  \n",
       "1        surgery       NaN           FT  \n",
       "2     immunology    online          NaN  \n",
       "3  physiotherapy    online          NaN  \n",
       "4        nursing       NaN          NaN  "
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkout the head of the candidates dataframe\n",
    "candidates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191942c5",
   "metadata": {},
   "source": [
    "<a id=\"preparing-the-data\"></a>\n",
    "## Preparing the Data\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbd111",
   "metadata": {},
   "source": [
    "In this section, both dataframes are prepared for future data visualization and prepared according to the algorithm utilised. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b08e9e",
   "metadata": {},
   "source": [
    "<a id=\"candidates-data-cleaning\"></a>\n",
    "### Candidates data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "3ad0bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all NaN values each dataframe column with a more appropriate value\n",
    "candidates[\"Experience\"].fillna(\"None\", inplace=True)\n",
    "candidates.fillna(\"N/A\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca8398",
   "metadata": {},
   "source": [
    "Let's now unify the StudyMode and StudyPattern columns, since they are less likely to be a decisive factor when calculating the relevance score of a candidate for a given job position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "374827d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['StudyProgram'] = candidates[[\"StudyMode\", \"StudyPattern\"]].agg('-'.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "a71388a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing values from the newly created StudyProgram column\n",
    "def clean(program):\n",
    "    program = program.split(\"-\")\n",
    "    study_mode, study_pattern = program[0], program[1]\n",
    "    \n",
    "    if study_mode == \"N/A\" and study_pattern == \"N/A\":\n",
    "        # assume student is campus, fulltime\n",
    "        return \"Campus-FT\"\n",
    "    \n",
    "    if study_mode == \"N/A\":\n",
    "        # assume student in oncampus\n",
    "        return f\"Campus-{study_pattern}\"\n",
    "    \n",
    "    if study_pattern == \"N/A\":\n",
    "        # assume student is fulltime\n",
    "        return f\"{study_mode.capitalize()}-FT\"\n",
    "    \n",
    "    # both entries are specified\n",
    "    return f\"{study_mode.capitalize()}-{study_pattern}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "6f0813eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the unified columns\n",
    "candidates.drop([\"StudyMode\", \"StudyPattern\"], axis=1, inplace=True)\n",
    " \n",
    "# handle null-values in the StudyProgram column\n",
    "candidates[\"StudyProgram\"] = candidates[\"StudyProgram\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "e8977712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fullname</th>\n",
       "      <th>Course</th>\n",
       "      <th>Score</th>\n",
       "      <th>Experience</th>\n",
       "      <th>StudyProgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ashley Harris</td>\n",
       "      <td>Doctor of Pharmacy (Pharm.D.)</td>\n",
       "      <td>96</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>Campus-FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria Harris</td>\n",
       "      <td>Master of Health Administration (M.H.A.)</td>\n",
       "      <td>77</td>\n",
       "      <td>surgery</td>\n",
       "      <td>Campus-FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thomas Boyer</td>\n",
       "      <td>Bachelor of Science in Biomedical Science (B.S.)</td>\n",
       "      <td>86</td>\n",
       "      <td>immunology</td>\n",
       "      <td>Online-FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holly Friedman</td>\n",
       "      <td>Bachelor of Science in Nursing (B.S.N.)</td>\n",
       "      <td>82</td>\n",
       "      <td>physiotherapy</td>\n",
       "      <td>Online-FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rebecca Cook DVM</td>\n",
       "      <td>Doctor of Pharmacy (Pharm.D.)</td>\n",
       "      <td>63</td>\n",
       "      <td>nursing</td>\n",
       "      <td>Campus-FT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fullname                                            Course  Score  \\\n",
       "0     Ashley Harris                     Doctor of Pharmacy (Pharm.D.)     96   \n",
       "1      Maria Harris          Master of Health Administration (M.H.A.)     77   \n",
       "2      Thomas Boyer  Bachelor of Science in Biomedical Science (B.S.)     86   \n",
       "3    Holly Friedman           Bachelor of Science in Nursing (B.S.N.)     82   \n",
       "4  Rebecca Cook DVM                     Doctor of Pharmacy (Pharm.D.)     63   \n",
       "\n",
       "      Experience StudyProgram  \n",
       "0      nutrition    Campus-FT  \n",
       "1        surgery    Campus-FT  \n",
       "2     immunology    Online-FT  \n",
       "3  physiotherapy    Online-FT  \n",
       "4        nursing    Campus-FT  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkout the head of candidates dataframe to see new dataframe\n",
    "candidates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aba133",
   "metadata": {},
   "source": [
    "<a id=\"jobs-data-cleaning\"></a>\n",
    "### Jobs Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbb29a",
   "metadata": {},
   "source": [
    " The round_to_closest function generalizes the MinScore into categories of multiples of 5. This may negatively influence the matchmaking process\n",
    " as some candidates may be wrongfully matched based on the rounded score, however it does simplify the matching process\n",
    " since there are less values the MinScore feature can take on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "21df1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_closest(x):\n",
    "    # extract the unitary digit\n",
    "    unitary = int(x) % 10\n",
    "    \n",
    "    # extract the decimal digit\n",
    "    decimal = int(x)//10\n",
    "      \n",
    "    # round up or down based on unitary digit\n",
    "    if unitary < 5: return decimal * 10\n",
    "    if unitary == 5: return x\n",
    "    else: return (decimal + 1) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "3bdcd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round the values in the MinScore column to the nearest 5%\n",
    "jobs['MinScore'] = jobs['MinScore'].apply(round_to_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "0fb10fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Field</th>\n",
       "      <th>MinScore</th>\n",
       "      <th>Positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dental Hygienist Internship</td>\n",
       "      <td>Jimenez, Carney and Foley</td>\n",
       "      <td>physiotherapy</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Registered Nurse Placement</td>\n",
       "      <td>Roth, Rose and Cross</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dental Hygienist Internship</td>\n",
       "      <td>Russell-Banks</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dental Hygienist Internship</td>\n",
       "      <td>Martin Ltd</td>\n",
       "      <td>immunology</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physician Internship</td>\n",
       "      <td>Lambert Group</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title                    Company          Field  \\\n",
       "0  Dental Hygienist Internship  Jimenez, Carney and Foley  physiotherapy   \n",
       "1   Registered Nurse Placement       Roth, Rose and Cross      nutrition   \n",
       "2  Dental Hygienist Internship              Russell-Banks      nutrition   \n",
       "3  Dental Hygienist Internship                 Martin Ltd     immunology   \n",
       "4         Physician Internship              Lambert Group      nutrition   \n",
       "\n",
       "   MinScore  Positions  \n",
       "0        80          7  \n",
       "1        80          8  \n",
       "2        90         10  \n",
       "3        80          3  \n",
       "4        80          9  "
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkout the head of the modified dataframe\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9219e",
   "metadata": {},
   "source": [
    "<a id=\"exploratory-data-viz\"></a>\n",
    "## Exploratory Data Analysis & Visualization\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b805e",
   "metadata": {},
   "source": [
    "In this section, the visualization libraries imported at the top of the notebook are put to use to plot both datasets and hopefully discover trends which could be relevant in the later stages of this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0724b94",
   "metadata": {},
   "source": [
    "<a id=\"candidates-data-analysis\"></a>\n",
    "### Candidates Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "82ec9f5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[452], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# histogram plot of scores split between different study-programs\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mScore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStudyProgram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:1432\u001b[0m, in \u001b[0;36mhistplot\u001b[1;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m estimate_kws \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   1422\u001b[0m     stat\u001b[38;5;241m=\u001b[39mstat,\n\u001b[0;32m   1423\u001b[0m     bins\u001b[38;5;241m=\u001b[39mbins,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     cumulative\u001b[38;5;241m=\u001b[39mcumulative,\n\u001b[0;32m   1428\u001b[0m )\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39munivariate:\n\u001b[1;32m-> 1432\u001b[0m     p\u001b[38;5;241m.\u001b[39mplot_univariate_histogram(\n\u001b[0;32m   1433\u001b[0m         multiple\u001b[38;5;241m=\u001b[39mmultiple,\n\u001b[0;32m   1434\u001b[0m         element\u001b[38;5;241m=\u001b[39melement,\n\u001b[0;32m   1435\u001b[0m         fill\u001b[38;5;241m=\u001b[39mfill,\n\u001b[0;32m   1436\u001b[0m         shrink\u001b[38;5;241m=\u001b[39mshrink,\n\u001b[0;32m   1437\u001b[0m         common_norm\u001b[38;5;241m=\u001b[39mcommon_norm,\n\u001b[0;32m   1438\u001b[0m         common_bins\u001b[38;5;241m=\u001b[39mcommon_bins,\n\u001b[0;32m   1439\u001b[0m         kde\u001b[38;5;241m=\u001b[39mkde,\n\u001b[0;32m   1440\u001b[0m         kde_kws\u001b[38;5;241m=\u001b[39mkde_kws,\n\u001b[0;32m   1441\u001b[0m         color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m   1442\u001b[0m         legend\u001b[38;5;241m=\u001b[39mlegend,\n\u001b[0;32m   1443\u001b[0m         estimate_kws\u001b[38;5;241m=\u001b[39mestimate_kws,\n\u001b[0;32m   1444\u001b[0m         line_kws\u001b[38;5;241m=\u001b[39mline_kws,\n\u001b[0;32m   1445\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1446\u001b[0m     )\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1450\u001b[0m     p\u001b[38;5;241m.\u001b[39mplot_bivariate_histogram(\n\u001b[0;32m   1451\u001b[0m         common_bins\u001b[38;5;241m=\u001b[39mcommon_bins,\n\u001b[0;32m   1452\u001b[0m         common_norm\u001b[38;5;241m=\u001b[39mcommon_norm,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1463\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:451\u001b[0m, in \u001b[0;36m_DistributionPlotter.plot_univariate_histogram\u001b[1;34m(self, multiple, element, fill, common_norm, common_bins, shrink, kde, kde_kws, color, legend, line_kws, estimate_kws, **plot_kws)\u001b[0m\n\u001b[0;32m    449\u001b[0m     kde_kws[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumulative\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m estimate_kws[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumulative\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    450\u001b[0m     log_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_scaled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_variable)\n\u001b[1;32m--> 451\u001b[0m     densities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_univariate_density\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_variable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommon_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommon_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkde_kws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# First pass through the data to compute the histograms\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_vars, sub_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, from_comp_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    462\u001b[0m \n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# Prepare the relevant data\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:349\u001b[0m, in \u001b[0;36m_DistributionPlotter._compute_univariate_density\u001b[1;34m(self, data_variable, common_norm, common_grid, estimate_kws, log_scale, warn_singular)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m singular:\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;66;03m# Convoluted approach needed because numerical failures\u001b[39;00m\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;66;03m# can manifest in a few different ways.\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m         density, support \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[0;32m    351\u001b[0m     singular \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_statistics.py:192\u001b[0m, in \u001b[0;36mKDE.__call__\u001b[1;34m(self, x1, x2, weights)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"Fit and evaluate on univariate or bivariate data.\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval_univariate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_bivariate(x1, x2, weights)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_statistics.py:161\u001b[0m, in \u001b[0;36mKDE._eval_univariate\u001b[1;34m(self, x, weights)\u001b[0m\n\u001b[0;32m    157\u001b[0m     density \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m    158\u001b[0m         kde\u001b[38;5;241m.\u001b[39mintegrate_box_1d(s_0, s_i) \u001b[38;5;28;01mfor\u001b[39;00m s_i \u001b[38;5;129;01min\u001b[39;00m support\n\u001b[0;32m    159\u001b[0m     ])\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     density \u001b[38;5;241m=\u001b[39m \u001b[43mkde\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m density, support\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\_kde.py:268\u001b[0m, in \u001b[0;36mgaussian_kde.evaluate\u001b[1;34m(self, points)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    267\u001b[0m output_dtype, spec \u001b[38;5;241m=\u001b[39m _get_output_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance, points)\n\u001b[1;32m--> 268\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_kernel_estimate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcho_cov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[:, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "sns.color_palette(\"PuOr\", as_cmap=True)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# histogram plot of scores split between different study-programs\n",
    "sns.histplot(\n",
    "    data=candidates, \n",
    "    x=\"Score\", \n",
    "    multiple=\"stack\", \n",
    "    hue='StudyProgram',\n",
    "    edgecolor=\".3\", \n",
    "    bins=50, \n",
    "    linewidth=.5, \n",
    "    kde=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc7ec5",
   "metadata": {},
   "source": [
    "From the above histogram, we can observe that the study-mode and study-pattern of a degree don't seem to affect a student's academic performance. This claim is backed up by the similarity in kernel-density line in the histogram above, as regardless of the study-program candidates tend to have a score between 60% and 85%, with some outliers scoring 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "# histogram plot of scores split between different study-programs\n",
    "chart = sns.countplot(\n",
    "    data=candidates, \n",
    "    x='Course', \n",
    "    palette='RdBu', \n",
    "    orient=\"v\"\n",
    ")\n",
    "\n",
    "# configure labels along the x-axis\n",
    "chart.set_xticklabels(\n",
    "    chart.get_xticklabels(),\n",
    "    rotation=60,\n",
    "    ha=\"right\",\n",
    "    rotation_mode='anchor'\n",
    ")\n",
    "\n",
    "None # hide the label objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ecd33",
   "metadata": {},
   "source": [
    "From the above countplot, we can see that for this particular batch of candidates, the majority are enrolled in Dentistry-related or Nursing/Medicine degree program, which requires that the corresponding batch of jobs must be rich in opportunities for these students, otherwise there's a risk of many of them going unmatched or matched to a less desirable job position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70479ed5",
   "metadata": {},
   "source": [
    "<a id=\"jobs-data-analysis\"></a>\n",
    "### Jobs Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# countplot of companies grouped according to the minimum scored required for their positions\n",
    "sns.countplot(\n",
    "    data=jobs, \n",
    "    x='MinScore', \n",
    "    palette='Set2',\n",
    "    orient=\"v\"\n",
    ")\n",
    "\n",
    "# display the background grid\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2cfa4d",
   "metadata": {},
   "source": [
    "From the above countplot, we can observe that most positions require candidates to have a minimum score between 80-90%, with around 12 outliers requiring particularly high score of 95%. \n",
    "\n",
    "Comparing this countplot with the one for the student scores, the following can be deduced :\n",
    "- Any student with a score below 70 risks of not getting matched to any internship\n",
    "- The students with a score of 100 are likely to be matched to the positions requiring a 95% score. \n",
    "\n",
    "It is worth noting that a greedy algorithm might fail to compute a stable match, so for example a student with 100% score might be matched to an internship requiring a 70% instead of a more suitable 95% internship. This would also cause students with a score of 70% having their position stolen from the better students, leaving them potentially unmatched. \n",
    "\n",
    "The *Gale-Shapley* algorithm aims to avoid this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee2ca5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# total number of positions per job grouped by field of job\n",
    "positions_per_field = jobs.groupby(\"Field\").sum(numeric_only=True)[\"Positions\"]\n",
    "\n",
    "# total number of students with experience grouped by field type\n",
    "candidates_per_field = candidates[\"Experience\"].value_counts()\n",
    "\n",
    "field_df = pd.DataFrame(positions_per_field).join(candidates_per_field)\n",
    "\n",
    "# create a column to analyse the difference in job supply and demand in each field\n",
    "field_df[\"Supply vs Demand\"] = field_df[\"Positions\"] - field_df[\"Experience\"]\n",
    "\n",
    "# show the dataframe\n",
    "field_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d6a04",
   "metadata": {},
   "source": [
    "By the above analysis we can conclude that for these particular datasets :\n",
    "\n",
    "- Physiotherapy has the highest demand to supply ratio, mostly due to the lower number of available job positions in the field\n",
    "- The remaining fields seem to be abundant in jobs\n",
    "- If this ratio were to be negative for a field, there's a risk of students remaining unmatched or having to accept a position in a field outside their preferred sector.\n",
    "\n",
    "Based on these observations alone, and excluding factors such as a job's MinScore vs a student's Score it appears as if *not* every student will be matched to an internship in their field of expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae217d4",
   "metadata": {},
   "source": [
    "<a id=\"gale-shapley\"></a>\n",
    "# Gale-Shapley Algorithm\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81751b7",
   "metadata": {},
   "source": [
    "The stable matching problem, in its most basic form, takes as input equal numbers of two types of participants (*n* job applicants and *n* employers, for example), and an ordering for each participant giving their preference for whom to be matched to among the participants of the other type. A matching pairs each participant of one type with a participant of the other type. A matching is not stable if:\n",
    "\n",
    "- There is an element *A* of the first matched set which prefers some given element *B* of the second matched set over the element to which *A* is already matched, and\n",
    "- *B* also prefers *A* over the element to which *B* is already matched.\n",
    "\n",
    "In other words, a matching is stable when there is no pair *(A, B)* where both participants prefer each other to their matched partners. If such a pair exists, the matching is not stable, in the sense that the members of this pair would prefer to leave the system and be matched to each other, possibly leaving other participants unmatched. A stable matching always exists, and the algorithmic problem solved by the GaleShapley algorithm is to find one.\n",
    "\n",
    "The GaleShapley algorithm involves a number of \"rounds\" or \"iterations\". In terms of job applicants and employers, it can be expressed as follows : \n",
    "\n",
    "- In each round, one or more employers with open job positions each make a job offer to the applicant they prefer, among the ones they have not yet already made an offer to.\n",
    "- Each applicant who has received an offer evaluates it against their current position (if they have one). If the applicant is not yet employed, or if they receive an offer from an employer they like better than their current employer, they accept the best new offer and become matched to the new employer (possibly leaving a previous employer with an open position). Otherwise, they reject the new offer.\n",
    "- This process is repeated until all employers have either filled their positions or exhausted their lists of applicants.\n",
    "\n",
    "The above algorithm is at the core of the implementation found below, however since the size of the candidates and jobs input sets is most likely to be different, this version of the algorithm only guarantees a portion of the matches computed to be stable, but aims to guarantee that every candidate receives an offer (given that there are enough offers to give out in the first place). \n",
    "\n",
    "This section is then concluded by motivating the adoption of an improved version of the algorithm, which takes the same idea of Gale-Shapley and tailors it to this specific problem and then compares both the runtime efficiency and percentage of jobs matched with the data gathered from the basic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35823381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# return a score to classify the compatibility of a candidate to a job\n",
    "def compute_compatibility(candidate, job):\n",
    "    compatibility_score = 0\n",
    "    \n",
    "    # check if student has similar experience\n",
    "    if candidate[\"Experience\"] == job[\"Field\"]: compatibility_score += 1\n",
    "    else: compatibility_score += 0.5\n",
    "    \n",
    "    # assign a reduced factor based on how far off the MinScore the candidate's score is\n",
    "    candidate_score = candidate[\"Score\"]\n",
    "    job_minscore = job[\"MinScore\"]\n",
    "    \n",
    "    # candidate will prefer job whose MinScore is closer to their achieved grade \n",
    "    if candidate_score <= 0.9 * job_minscore or candidate_score >= 1.1 * job_minscore: compatibility_score += 1\n",
    "    elif candidate_score <= 0.75 * job_minscore or candidate_score >= 1.25 * job_minscore: compatibility_score += 0.5\n",
    "    else: compatibility_score += 0.25\n",
    "        \n",
    "    # account for a candidate preferring location, pay, company title etc.\n",
    "    compatibility_score += random.random()\n",
    "    \n",
    "    return round(compatibility_score, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b09b5f",
   "metadata": {},
   "source": [
    "<a id=\"benchmark-decorator\"></a>\n",
    "## Performance Benchmark Decorator\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "# decorator function to measure the time performance of function fn\n",
    "def benchmark(fn):\n",
    "\n",
    "    @wraps(fn)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \n",
    "        # save time snapshots before and after function call\n",
    "        start_time = time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        end_time = time()\n",
    "        \n",
    "        # display time elapsed and return result of fn call\n",
    "        print(f'Executed {fn.__name__}, Time Elapsed : {round(end_time - start_time, 3)} seconds')\n",
    "        return result\n",
    "    \n",
    "    # return the wrapper function\n",
    "    return wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4194807",
   "metadata": {},
   "source": [
    "We create a computability matrix, where each row is a student and each column a job position. \n",
    "The entry at position *(i, j)* is the compatibility score between student *i* and job *j*. \n",
    "The table will have *m x n* entries, where *m* is the number of candidates and *n* the number of representative jobs available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5cc2d2",
   "metadata": {},
   "source": [
    "<a id=\"control-variables\"></a>\n",
    "## Configuring Control Variables\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1043fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control variables used by the Gale-Shapley algorithm\n",
    "total_job_titles = len(jobs)\n",
    "total_jobs_available = jobs.loc[:total_job_titles, \"Positions\"].sum()\n",
    "total_candidates = len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921557e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the tqdm module to implement an real-time progress bar for the loop below\n",
    "import sys\n",
    "from time import sleep\n",
    "from tqdm import trange\n",
    "\n",
    "# populate the compatibility matrix using the jobs and candidates dataframe\n",
    "@benchmark\n",
    "def populate_compatibility_matrix(matrix, candidates, jobs):\n",
    "    for i in trange(len(candidates), file=sys.stdout, colour='GREEN'):\n",
    "        for j in range(len(jobs)): \n",
    "            matrix.loc[(i, j)] = compute_compatibility(candidates.loc[i], jobs.loc[j])\n",
    "        sleep(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3c837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute the preference matrix for the given dataframes\n",
    "def compute_preference_matrix(candidates, jobs):\n",
    "    \n",
    "    # use jobs and candidates indices instead of fullname and company as it guarantees uniqueness\n",
    "    jobs_indices = [ i for i in range(len(jobs)) ]\n",
    "    candidate_indices = [ i for i in range(len(candidates)) ]\n",
    "\n",
    "    # create the compatibility dataframe/matrix\n",
    "    compatibility = pd.DataFrame(0, index=candidate_indices, columns=jobs_indices)\n",
    "    compatibility = compatibility.rename_axis(index='Candidate IDs', columns='Job IDs')\n",
    "\n",
    "    # populate the compatibility matrix\n",
    "    print(\"Populating the Compatibility matrix...\")\n",
    "    populate_compatibility_matrix(compatibility, candidates, jobs)\n",
    "    print(\"Population operation completed.\")\n",
    "    \n",
    "    return compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a296fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a list of jobs, return the index of a job with offers to still give out, or -1 if none are found\n",
    "def find_job(company_ids):\n",
    "    for i in range(len(company_ids)):\n",
    "        if company_ids[i] >= 0: return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suitable_candidates(company_index, compatibility_matrix):\n",
    "    # retrieve the column with key of company_index from the compatibility matrix\n",
    "    suit_cands = [(i, compatibility_matrix[company_index][i]) for i in range(len(compatibility_matrix[company_index]))]\n",
    "    \n",
    "    # sort the candidates by their compatibility score descending, so company makes offer to most relevant candidates first\n",
    "    suit_cands.sort(key = lambda x: x[1])\n",
    "    \n",
    "    return suit_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all the offers made in a user-readable format\n",
    "def format_pairings(offers):\n",
    "    for candidate_id in offers.keys():\n",
    "        candidate_name = candidates.loc[candidate_id, \"Fullname\"]\n",
    "        job_title = \"N/A\" if offers[candidate_id][0] == None else jobs.loc[offers[candidate_id][0], \"Title\"]\n",
    "        print(f'{candidate_name} -> {job_title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67111fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark\n",
    "# run the gale-shapley algorithm on the input candidates and jobs sets\n",
    "def gale_shapley(company_ids, offers, compatibility_matrix, available_positions, max_iterations=10000):\n",
    "    \n",
    "    # keep track of number of companies fulfilled at each iteration\n",
    "    fulfillments = []\n",
    "    iterations = 0\n",
    "    \n",
    "    # trigger time-out when algorithm reaches a stagnant point\n",
    "    while iterations < max_iterations:\n",
    "        \n",
    "        # find a company with job offers to give out\n",
    "        company_id = find_job(company_ids)\n",
    "        \n",
    "        # stop condition when all companies have given out jobs\n",
    "        if company_id == -1: break\n",
    "            \n",
    "        # job J with positions still to fill-out\n",
    "        j = company_ids[company_id]\n",
    "        \n",
    "        # find most compatible candidate who company j has not offered a job to yet\n",
    "        comps = suitable_candidates(j, compatibility_matrix)\n",
    "        \n",
    "        # check whether all students reject this company\n",
    "        all_reject = True\n",
    "        \n",
    "        for candidate in comps:\n",
    "            candidate_id = candidate[0]\n",
    "            \n",
    "            # make an offer to this candidate\n",
    "            if j not in offers[candidate_id][1]:\n",
    "                \n",
    "                # check if candidate has no offer yet\n",
    "                if offers[candidate_id][0] == None:\n",
    "                    \n",
    "                    # make the offer \n",
    "                    offers[candidate_id][0] = j\n",
    "                    \n",
    "                    # change reject status\n",
    "                    all_reject = False\n",
    "                    \n",
    "                    # reduce number of jobs available for job j\n",
    "                    available_positions[j] -= 1\n",
    "                    \n",
    "                    # check number of positions \n",
    "                    if available_positions[j] == 0:\n",
    "                        \n",
    "                        # all positions have been filled\n",
    "                        company_ids[j] = -1\n",
    "                        break\n",
    "                        \n",
    "                    # make sure this company cannot make another offer to the same candidate\n",
    "                    offers[candidate_id][1].append(company_id)\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    # make offer, then candidate chooses the company they compare best with\n",
    "                    k = offers[candidate_id][0] # id of company candidate has an offer from\n",
    "                    prev_offer_score = compatibility_matrix.loc[candidate_id, k]\n",
    "                    curr_offer_score = compatibility_matrix.loc[candidate_id, j]\n",
    "                    \n",
    "                    if curr_offer_score > prev_offer_score:\n",
    "                        \n",
    "                        # candidate accepts new offer \n",
    "                        offers[candidate_id][0] = j\n",
    "                        \n",
    "                        # change reject status\n",
    "                        all_reject = False\n",
    "                        \n",
    "                        # old offer now becomes free\n",
    "                        available_positions[k] += 1\n",
    "                        \n",
    "                        # current job position is not available to other candidates, so decrement\n",
    "                        available_positions[j] -= 1\n",
    "                        \n",
    "                        # check if old job has new positions free\n",
    "                        if available_positions[k] == 1: company_ids[k] = k    \n",
    "                            \n",
    "                        # check if new job has been completely filled out\n",
    "                        if available_positions[j] == 0: company_ids[j] = -1  \n",
    "                            \n",
    "                    else:\n",
    "                        # candidate rejects new offer - add this company to the banned list for this candidate\n",
    "                        offers[candidate_id][1].append(company_id)\n",
    "                    break\n",
    "                    \n",
    "        # if company rejected by all students, then remove it from being considered next\n",
    "        if all_reject: company_ids[j] = -1\n",
    "            \n",
    "        # save number of fulfilled companies after current iteration\n",
    "        fulfillments.append(company_ids.count(-1))\n",
    "        iterations += 1\n",
    "        \n",
    "    if iterations >= max_iterations: print(\"Termination due to time-out\")\n",
    "        \n",
    "    return [offers, fulfillments]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1521e",
   "metadata": {},
   "source": [
    "<a id=\"performance-accuracy\"></a> \n",
    "## Performance Metrics & Evaluation\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ef71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# run the Gale-Shapley algorithm for a range of candidates and jobs\n",
    "def run_gale_shapley(number_of_candidates, number_of_jobs, verbose):\n",
    "    \n",
    "        start_time = time()\n",
    "        \n",
    "        print(f\"\\nRunning Gale-Shapley for {number_of_candidates} candidates and {number_of_jobs} jobs...\")\n",
    "    \n",
    "        # reduce size of candidates and jobs dataframes \n",
    "        candidates_dataframe = candidates.loc[:number_of_candidates]\n",
    "        jobs_dataframe = jobs.loc[:number_of_jobs]\n",
    "        \n",
    "        # compute the compatibility matrix\n",
    "        compute_matrix_start_time = time()\n",
    "        compatibility_matrix = compute_preference_matrix(candidates_dataframe, jobs_dataframe)\n",
    "        compute_matrix_elapsed_time = round(time() - compute_matrix_start_time, 1)\n",
    "        \n",
    "        # keeps track of companies with job offers to still give out \n",
    "        company_ids = compatibility_matrix.columns.tolist()\n",
    "\n",
    "        # keeps track of the offers made so far, candidate_id : (current_offer_company_id, [refusing_company_id1, ...])\n",
    "        offers = { candidate_id : [None, []] for candidate_id in range(number_of_candidates + 1) }\n",
    "\n",
    "        # keeps track of the number of positions left per job\n",
    "        available_positions = [ jobs_dataframe.loc[job_id, \"Positions\"] for job_id in range(len(jobs_dataframe)) ]\n",
    "        \n",
    "        # compute the total number of positions across number_of_jobs jobs\n",
    "        total_jobs = sum(available_positions)\n",
    "        \n",
    "        # run the Gale-Shapley algorithm between the jobs and candidates dataset\n",
    "        offers, fulfillments = gale_shapley(company_ids, offers, compatibility_matrix, available_positions)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nOffers computed for {number_of_candidates} candidates and {number_of_jobs} jobs > \", format_pairings(offers))\n",
    "        \n",
    "        # store number of input candidates and input jobs alongside number of matches constructed by gale_shapley\n",
    "        candidates_with_offers = len([ candidate_id for candidate_id in offers.keys() if offers[candidate_id] != 1 ])\n",
    "        jobs_assigned = candidates_with_offers\n",
    "        percentage_metrics = (number_of_candidates, total_jobs, min(candidates_with_offers, number_of_candidates), min(total_jobs, jobs_assigned))\n",
    "        \n",
    "        # compute iterations of this algorithm call\n",
    "        iterations = len(fulfillments)\n",
    "        \n",
    "        # compute time of execution\n",
    "        elapsed_time = round(time() - start_time, 2)\n",
    "        \n",
    "        return (fulfillments[::-1], iterations, percentage_metrics, elapsed_time, compute_matrix_elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcd1c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute lineplots for the performance of the algorithm over a range of candidates and jobs\n",
    "performance_data = [ run_gale_shapley(i, j, verbose=False) for j in range(25, 125, 25) for i in range(100, 1100, 100) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad698937",
   "metadata": {},
   "source": [
    "<a id=\"jobs-iterations\"></a>\n",
    "### Jobs Remaining vs Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bbacf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set the style for the plot\n",
    "plt.figure(figsize=(21, 10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# configure multi-level index for the dataframe, such that each 100 candidates have 25, 50, 75, 100 jobs \n",
    "iterations_outer_index = [ i for i in range(100, 1100, 100) ]\n",
    "iterations_inner_index = [ i for i in range(25, 125, 25) ]\n",
    "iterations_multi_index = pd.MultiIndex.from_product([iterations_outer_index, iterations_inner_index], names=['Candidates', 'Jobs Advertised'])\n",
    "\n",
    "# create a DataFrame with the MultiIndex for the Iterations data\n",
    "iterations_data = {\n",
    "    'Iterations' : [ data_tuple[1] for data_tuple in performance_data ],\n",
    "    'Jobs Remaining' : [ data_tuple[0] for data_tuple in performance_data ],\n",
    "}\n",
    "\n",
    "iterations_df = pd.DataFrame(data=iterations_data, index=iterations_multi_index)\n",
    "\n",
    "for i in range(len(iterations_df)):\n",
    "    curr_row = iterations_df.iloc[i]\n",
    "    \n",
    "    # convert the 'Jobs Remaining' column to a list of lists\n",
    "    jobs_remaining_list = curr_row['Jobs Remaining']\n",
    "\n",
    "    # create a DataFrame for the current row\n",
    "    df_curr_row = pd.DataFrame(\n",
    "        {'Iterations': range(1, len(jobs_remaining_list) + 1), \n",
    "         'Jobs Remaining': jobs_remaining_list}\n",
    "    )\n",
    "    \n",
    "    # create the line plot\n",
    "    sns.lineplot(data=df_curr_row, x='Iterations', y='Jobs Remaining')\n",
    "    \n",
    "# set plot labels and title\n",
    "plt.title('Jobs Remaining vs Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Jobs Remaining')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2b9a2",
   "metadata": {},
   "source": [
    "From the above visualization, we can see that the algorithm behaves in a logarithmic way, and in all cases either all jobs are assigned to candidates or all jobs are considered at least once by the algorithm. \n",
    "\n",
    "I believe this behaviour is a result of the *all_reject* control variable in the *gale_shapley* methods, as this ensures that if a job position *j* is rejected by all candidates on iteration *x* of the algorithm, then it will be discarded by any future iteration *y*, so *j* has one \"pass\" of the entire set of candidates to be assigned, otherwise it becomes a reject job.\n",
    "\n",
    "In conclusion, the use of a single-scan approach to assign job *j* guarantees an extremely fast algorithm [1], however the % of jobs assigned tends to be unacceptably low, as indicated by the chart below titled *Jobs Assigned % per Number of Candidates*, but an optimization can be applied which guarantees 100% matching rate, at the expense of a slight increase in runtime. See the subsection titled *Improving the percentage of jobs assigned* for more details.\n",
    "\n",
    "[1] In fact most of the computational time of the *run_gale_shapley* driver function is taken up computing the compatibility matrix for the input datasets, so adopting memoization techniques to expand the matrix based on the previous one rather than repopulating it each time would drastically improve the speed of this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82f84c",
   "metadata": {},
   "source": [
    "<a id=\"runtime-inputsize\"></a> \n",
    "### Run-time vs Input size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6532f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "time_outer_index = [ i for i in range(100, 1100, 100) ]\n",
    "time_inner_index = [ i for i in range(25, 125, 25) ]\n",
    "time_multi_index = pd.MultiIndex.from_product(\n",
    "    [time_outer_index, time_inner_index], \n",
    "    names=['Candidates', 'Jobs Advertised']\n",
    ")\n",
    "\n",
    "# create a DataFrame with the MultiIndex for the Time Elapsed data\n",
    "time_data = {'Time Elapsed' : [ data_tuple[3] for data_tuple in performance_data ] }\n",
    "time_df = pd.DataFrame(data=time_data, index=time_multi_index)\n",
    "\n",
    "# plot the above dataframe\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "time_chart = sns.lineplot(\n",
    "    data=time_df.reset_index(), \n",
    "    x='Jobs Advertised', \n",
    "    y='Time Elapsed', \n",
    "    palette='Paired', \n",
    "    hue='Candidates', \n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "plt.title('run_gale_shapley - Time Elapsed')\n",
    "plt.xlabel('Jobs')\n",
    "plt.ylabel('Time Elapsed (s)')\n",
    "\n",
    "sns.move_legend(time_chart, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1e3de",
   "metadata": {},
   "source": [
    "The above chart suggests that the *run_gale_shapley* method is linear in nature, as the it seems to scale linearly to the increasing number of jobs and is independant of the size of the candidates pool. This makes sense, considering that the underlying algorithm will iterate *n x m* times where *n* is the size of the jobs input set and *m* the size of the candidates set. \n",
    "\n",
    "Note however that if *m* approaches *n* the overall time complexity of the algorithm will be *O(n^2)*, but in such case the algorithm would likely terminate before going through *n^2* iterations, since all job offers will be given out before this is the case.\n",
    "\n",
    "The runtime of this function can be further improved by speeding up the time taken to compute the compatibility matrix and sorting the Series of candidates associated with job *j*, such that the most compatible candidate will be likely to be matched earlier with their 1st choice job position and excluded from the next iteration, as there's no reason for them to give up their current offer for another one guaranteed to be less satisfactory. Formally, if candidate *c* is the one at index 0 in the Series for job *j*, then *compatibility(c) >= compatibility(c')*, where *c'* is at index 1+ so assigning job *j* to *c'* would not benefit neither *c* or *j* and therefore this approach to the pairing problem guaranteed a stable-match [1] in the first generation of the algorithm. [2]\n",
    "\n",
    "[1] The definition of a stable-match can be found at the top of the Gale-Shapley section.\n",
    "[2] To better understand the term \"generation\", please consult the section where I work on improving the time efficiency and accuracy of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32b133",
   "metadata": {},
   "source": [
    "<a id=\"percentage\"></a>\n",
    "### Percentage of Offers vs Input Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4be4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# create multi-level index for the dataframe\n",
    "percentage_outer_index = [ i for i in range(100, 1100, 100) ]\n",
    "percentage_inner_index = [ i for i in range(25, 125, 25) ]\n",
    "percentage_multi_index = pd.MultiIndex.from_product(\n",
    "    [percentage_outer_index, percentage_inner_index], \n",
    "    names=['Candidates', 'Jobs Advertised']\n",
    ")\n",
    "\n",
    "# create a dataframe with the MultiIndex for the Assignment percentage data\n",
    "percentage_data = {\n",
    "    'Assigned Candidates %' : [ round(100 * data_tuple[2][2]/data_tuple[2][0], 1) for data_tuple in performance_data ],\n",
    "    'Assigned Jobs %' : [ round(100 * data_tuple[2][3]/data_tuple[2][1], 1) for data_tuple in performance_data ]\n",
    "            }\n",
    "\n",
    "percentage_df = pd.DataFrame(data=percentage_data, index=percentage_multi_index)\n",
    "\n",
    "# Plot the above dataframe\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "percentage_chart = sns.lineplot(\n",
    "    data=percentage_df.reset_index(), \n",
    "    x='Jobs Advertised', \n",
    "    y='Assigned Jobs %', \n",
    "    palette='Paired', \n",
    "    hue='Candidates', \n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "plt.title('Jobs Assigned % per Number of Candidates')\n",
    "plt.xlabel('Jobs')\n",
    "plt.ylabel('Assigned Jobs %')\n",
    "\n",
    "sns.move_legend(percentage_chart, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad1cf2",
   "metadata": {},
   "source": [
    "From the above graph, we can conclude that the percentage of jobs assigned to candidates varies drastically between each of the inputs, but an increase in percentage can be observed in all test cases except for input sets of 300 or 800 candidates. This behaviour is most likely to be caused by characteristics specific to the input datasets rather than the decision making within the algorithm. \n",
    "\n",
    "Overall it appears as if the percentage of jobs assigned tends to increase as the number of jobs available increases, and this behaviour seems to be consistent regarless of the size of the candidate pool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ecbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what the percentage_df indicates about the performance of the algorithm\n",
    "percentage_df[percentage_df[\"Assigned Jobs %\"] < 100.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026bb3b",
   "metadata": {},
   "source": [
    "<a id=\"runtime-improvements\"></a>\n",
    "## Run-time Improvements\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741a667",
   "metadata": {},
   "source": [
    "The main aspects this improvement aims to target is to reduce the upfront cost of computing the compatibility matrix by adopting caching techniques and reduce the overall runtime of the matching process. We will explore three different approaches to populating the compatibility matrix for a specific set of inputs and then plot the runtimes to see if caching the previously computed submatrix yields faster execution time.\n",
    "\n",
    "Reducing the time to compute the matrix can be done by adding rows and columns to a matrix computed for a smaller dataset. For example, the compatibility matrix for an input of 500 candidates and 50 jobs can be computed by populating the compatibility score for the extra 100 candidates against the same 50 jobs instead of re-computing the score for the same 400 candidates again, and then for the 100 new candidates. Note that this approch only improves the time complexity and doesn't affect the space complexity, moreover the time complexity to compute the matrix with the current approach is *O((m x n) ^ 2)* due to the repeated computations, but the amortised time complexity with memoization will be *O(m x n)*, since we are looking up the values from the previous set of inputs in *O(1)* time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the entire dataframes for quick referencing when working with them\n",
    "candidates_df = candidates\n",
    "jobs_df = jobs\n",
    "\n",
    "# define a dataframe to represent the compatibility matrix, whose size can be at most m * n\n",
    "matrix = pd.DataFrame(\n",
    "    -1, \n",
    "    index=[ i for i in range(len(candidates_df)) ],\n",
    "    columns=[ j for j in range(len(jobs_df)) ]).rename_axis(index='Candidate IDs', columns='Job IDs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0beca1",
   "metadata": {},
   "source": [
    "<a id=\"compute-matrix-v2\"></a>\n",
    "### Modification - Computing the Compatibility matrix with pre-population assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabb295",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# populate subsection of matrix confined by rows last_row : curr_row and columns last_col : curr_col\n",
    "def populate_compatibility_matrix_v2(candidates, jobs, matrix, curr_row, last_row, curr_col, last_col):\n",
    "    for i in range(curr_row - last_row, curr_row):\n",
    "        for j in range(curr_col - last_col, curr_col):\n",
    "            matrix.loc[i, j] = compute_compatibility(candidates.loc[i], jobs.loc[j])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# use the previously computed performance data from the run_gale_shapley method\n",
    "populate_compatibility_matrix_v1_times = [ data_point[4] for data_point in performance_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd426858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test time elapsed to populate matrix section by section, assuming pre-population\n",
    "populate_compatibility_matrix_v2_times = []\n",
    "for i in range(100, 1001, 100):\n",
    "    start_time = time()\n",
    "    for j in range(25, 101, 25):\n",
    "        matrix = populate_compatibility_matrix_v2(candidates_df, jobs_df, matrix, i, 100, j, 25)\n",
    "    elapsed_time = round(time() - start_time, 1)\n",
    "    populate_compatibility_matrix_v2_times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb096c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_compatibility_matrix_v3_times = []\n",
    "# compute the entire matrix in one scan, computation done beforehand instead of on-demand\n",
    "start_time = time()\n",
    "for i in range(len(candidates)):\n",
    "    for j in range(len(jobs)):\n",
    "        matrix.loc[i, j] = compute_compatibility(candidates.loc[i], jobs.loc[j])\n",
    "elapsed_time = round(time() - start_time, 1)\n",
    "\n",
    "# we append the result of running the entire loop 10 times (100, 200... 1000 candidates) each time, \n",
    "# since the inner loop runs in 0.1s on average\n",
    "for i in range(10):\n",
    "    populate_compatibility_matrix_v3_times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07b3cf",
   "metadata": {},
   "source": [
    "<a id=\"testing-population-methods\"></a>\n",
    "### Testing Matrix Population Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter times from the performance_data received from the run_gale_shapley method to contain time to compute all jobs per 100 candidates\n",
    "filtered_populate_compatibility_matrix_v1_times = [ round(sum(populate_compatibility_matrix_v1_times[i : i + 3]),1) for i in range(len(populate_compatibility_matrix_v1_times)) if i % 4 == 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = {\n",
    "    'Candidates' : [ i for i in range(100, 1001, 100) ],\n",
    "    'populator_v1' : filtered_populate_compatibility_matrix_v1_times,\n",
    "    'populator_v2' : populate_compatibility_matrix_v2_times,\n",
    "    'populator_v3' : populate_compatibility_matrix_v3_times,\n",
    "}\n",
    "\n",
    "# create a dataframe to store the time taken by each approach for x number of candidates \n",
    "populate_compatibility_matrix_times_df = pd.DataFrame(time_data)\n",
    "\n",
    "populate_compatibility_matrix_times_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce091e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# plot the lines\n",
    "sns.lineplot(x='Candidates', y='populator_v1', data=populate_compatibility_matrix_times_df, label='populator_v1', marker='o')\n",
    "sns.lineplot(x='Candidates', y='populator_v2', data=populate_compatibility_matrix_times_df, label='populator_v2', marker='o')\n",
    "sns.lineplot(x='Candidates', y='populator_v3', data=populate_compatibility_matrix_times_df, label='populator_v3', marker='o')\n",
    "\n",
    "# set labels and title\n",
    "plt.xlabel('Candidates')\n",
    "plt.ylabel('Time Elapsed (s)')\n",
    "plt.title('Time elapsed to compute Compatibility Matrix by populator version')\n",
    "\n",
    "# show the legend\n",
    "plt.legend()\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3af96",
   "metadata": {},
   "source": [
    "From the above line plot, we can conclude that the fastest method to compute the Compatibility Matrix for a problem instance is the one outlined by the *populate_compatibility_matrix_v2* method, where for each index input we search for the last populated row index, say *i* and within that row we search for the last populated column index, say *j*.\n",
    "The slowest approach was the one adopted by the *populate_compatibility_matrix_v1* method, since it re-computes the entire matrix every time a matching query has to be carried out.\n",
    "\n",
    "In this case, these indices are precisely 100 rows above and 25 columns to the left of each input point, since these are computed by nested for loops with step size of 100 and 25 respectively. **Recall** however, that this method's efficiency only works if the search for *i* or *j* terminates before reaching the beginning of the first row or column, namely indices 0. Otherwise, this approach yields *O(n^2)* complexity since for each index *i* or *j* we search backwards *i* and *j* rows/columns.\n",
    "\n",
    "To compute the entire matrix ahead, around 35-40 seconds were required. In conclusion, when we are trying to improve the time complexity (notice that the space complexity is the same across all instances, namely *O(m x n)* for *m* candidates and *n* jobs) : \n",
    "- If we are trying to populate a subsection of a matrix partially populated, it is slightly more efficient to compute the subsection based on the assumption that the previous subsection has been populated\n",
    "- If we have a lot of matching queries to execute and the maximum number of candidates and jobs is known in advance, compute the entire matrix in *O(m x n)* time and access the compatibility values in *O(1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668817f3",
   "metadata": {},
   "source": [
    "<a id=\"improving-offer-pecentage\"></a>\n",
    "## Improving the percentage of offers computed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c17f62",
   "metadata": {},
   "source": [
    "Except for some edge cases, the percentage of jobs fully exhausted (i.e. all of its positions are filled) is usually less than 100%. This might be due to several factors, such as a particular job being \"banned\" from a succeeding iteration of the algorithm if no candidate was willing to accept the offer made during the current round. \n",
    "\n",
    "An approach which could be used to maximize the percentage of jobs assigned relies on one simple concept, for us humans at least, which is that a candidate in a competitive pool would much rather have an offer from a company which was not their first choice rather than end up with no offers at the end of the recruitment cycle due to their preferences. \n",
    "\n",
    "In the scope of the Gale-Shapley algorithm, this concept would be implemented through simple modifications.\n",
    "\n",
    "1. The *compute_compatibility(candidate, job)* function will now take in a 3rd parameter *alpha* which denotes how many attributes are included in the computation of the score. For example is *alpha* = 0, then all attributes are involved and if *alpha* = n (number of attributes) then the score output defaults to 0.0\n",
    "\n",
    "2. The *run_gale_shapley(...)* function will now do the following : \n",
    "    - set *alpha* to 0\n",
    "    - execute Gale-Shapley on the input sets\n",
    "    - exclude the candidates and jobs members of an offer from the input sets\n",
    "    - increase *alpha* by 1 \n",
    "    - repeat Gale-Shapley on this filtered data set until either input set is empty (i.e. jobs = None or candidates = None).\n",
    "    \n",
    "With these modifications in place, a 100% offer-rate is guaranteed, since the compatibility scores of each (candidate, job) pair will eventually converge to 0.0 and the algorithm will assign the remaining entites on a first-come, first-serve basis until the stopping condition is met.\n",
    "\n",
    "Note however, that candidates and jobs matched in round *i* are more likely to be satisfied with their pairing than two entites matched in round *i+1* since their compatibility score is likely to be close to the one of other entities in the same input sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0c048",
   "metadata": {},
   "source": [
    "<a id=\"gale-shapley-conclusion\"></a>\n",
    "## Conclusion\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703209ae",
   "metadata": {},
   "source": [
    "In conclusion, the variant of the Gale-Shapley algorithm implemented in this subsection is a reliable approach to automating the matchmaking process between a set of candidates to a set of job offers. The main advantage of using this offline pairing strategy is that a stable match is guaranteed between the two input sets, with candidates left without an offer being the least qualified for the entire pool of available jobs since by nature, the algorithm will compare the current job it is trying to assign to the offer held by the candidate it is currently considering and assign them the job with maximal compatibility score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545e49b",
   "metadata": {},
   "source": [
    "<a id=\"linear-svm\"></a> <br>\n",
    "# Linear Support Vector Machines\n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150926ab",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "<a id=\"svm-strategy\"></a> <br>\n",
    "### Proposed Strategy for the SVM model\n",
    "\n",
    "- The task at hand is to create a model capable of predicting whether an offer between a candidate and a job is likely to be accepted by the candidates involved. If the SVM classifier redeems the offer as likely to be accepted, then it will select it for further operations, otherwise a rejection will happen.\n",
    "\n",
    "- I plan on training the model using past offer data, instead of using the compatibility function like the one used in the Gale-Shapley section. Each entry in the training dataframe will have the following (or similar) format : \n",
    "\n",
    "| Candidate ID | Job ID      | Candidate GPA | Job Minimum GPA | Compatibility Score | ...          | Offer Accepted |\n",
    "| -----------  | ----------- | -----------   | -----------     | -----------         | -----------  | -----------    |\n",
    "| 58           | 22          | 78            | 63              | 2.3                 | ...          | YES            |\n",
    "| 29           | 68          | 66            | 80              | 1.6                 | ...          | NO             |\n",
    "\n",
    "- The model will then classify an incoming offer based on the numerical features from the dataframe, such as GPAs and Compatibility score and label the offer as \"promising\" or \"not-promising\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed3be",
   "metadata": {},
   "source": [
    "<a id=\"svm-data-analysis\"></a> <br>\n",
    "### Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684136f",
   "metadata": {},
   "source": [
    "In this subsection, I will generate a plot of the three main features I plan on using to develop a Support Vector Classifier : the candidate's score (or GPA), the job's minimum score required, and the compatibility score of the candidate and job based on non-numerical attributes, named *alpha*.\n",
    "In addition, I will plot a heatmap of the compatibility matrix generated for a particular input set to see if there are any patterns hidden within which could help gather insight while developing the SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f71bc23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# trim the candidates and jobs dataset for plotting purposes \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m svm_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mcandidates\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m25\u001b[39m]\n\u001b[0;32m      3\u001b[0m svm_jobs \u001b[38;5;241m=\u001b[39m jobs[:\u001b[38;5;241m25\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# compute preference matrix for the initial candidates and jobs input sets, after data cleaning\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'candidates' is not defined"
     ]
    }
   ],
   "source": [
    "# trim the candidates and jobs dataset for plotting purposes \n",
    "svm_candidates = candidates.iloc[:25]\n",
    "svm_jobs = jobs[:25]\n",
    "\n",
    "# compute preference matrix for the initial candidates and jobs input sets, after data cleaning\n",
    "matrix = compute_preference_matrix(svm_candidates, svm_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf09d77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.heatmap(matrix, cmap=\"plasma\", annot=True, linewidth=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9c962",
   "metadata": {},
   "source": [
    "From the heatmap above, it is clear that for the subset of jobs and candidates selected, the compatibility score tends to be between the 2.50 and 1.50, with outlier pairs looking to be evenly spread between the extremes of the color scale. This was expected, considering that the data in the candidates and jobs dataframe was randomly generated, however this plot makes me believe that most of the matches computed will not be stable (click [**here**](#gale-shapley) for a refresher) but that is expected since most matches computed in a dynamic environment like the real world tend to follow this pattern.\n",
    "\n",
    "While this heatmap is informative, plotting the features involved in three dimensions might help us deduce more information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53458ab6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((100, 10))\n",
    "\n",
    "# populate matrix with compatibility score\n",
    "for i in range(100):\n",
    "    for j in range(10):\n",
    "        matrix[i, j] = compute_compatibility(candidates.loc[i], jobs.loc[j])\n",
    "\n",
    "x_indices = np.arange(matrix.shape[0])\n",
    "y_indices = np.arange(matrix.shape[1])\n",
    "X, Y = np.meshgrid(x_indices, y_indices)\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=X.flatten(),\n",
    "    y=Y.flatten(),\n",
    "    z=matrix.flatten(),\n",
    "    mode='markers',\n",
    "    marker=dict(size=5,color=matrix.flatten(), colorscale='Plasma',  opacity=0.8)\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Candidate ID vs Job ID vs Compatibility',\n",
    "    scene=dict(xaxis_title='Candidate ID', yaxis_title='Job ID', zaxis_title='Compatibility'),\n",
    "    autosize=False,\n",
    "    width=700,\n",
    "    height=700,\n",
    "    margin=dict(l=65, r=50, b=65, t=90)\n",
    ")\n",
    "\n",
    "# plot the green plane\n",
    "x_range = np.linspace(X.min(), X.max(), num=2)\n",
    "y_range = np.linspace(Y.min(), Y.max(), num=2)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "zz = np.full(xx.shape, 1.4)\n",
    "fig.add_trace(go.Surface(x=xx, y=yy, z=zz, colorscale=[[0, 'rgba(0, 204, 102, 0.6)'], [1, 'rgba(0, 204, 102, 0.6)']], showscale=False))\n",
    "\n",
    "# plot the blue plane\n",
    "x_range = np.linspace(X.min(), X.max(), num=2)\n",
    "y_range = np.linspace(Y.min(), Y.max(), num=2)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "zz = np.full(xx.shape, 2.5) \n",
    "fig.add_trace(go.Surface(x=xx, y=yy, z=zz, colorscale=[[0, 'rgba(0, 153, 255, 0.6)'], [1, 'rgba(0, 153, 255, 0.6)']], showscale=False))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74617fe",
   "metadata": {},
   "source": [
    "From the above 3D scatterplot, we can observe that most of the compatibility scores for this set of inputs score between 1.5 and 2.5 *green-blue plane*, with outliers being found in the bottom 30% for the vast majority and a smaller portion (about 16%) found in the 2.5-3 band. This plot indicates that most of the matches computed will not be [**stable**](#gale-shapley) since the compatibility score tend to be distributed towards the lower end of the 0.0 to 3.0 range of possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d3c82",
   "metadata": {},
   "source": [
    "<a id=\"svm-df-setup\"></a> <br>\n",
    "### Constructing the Dataframe for SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087646a",
   "metadata": {},
   "source": [
    "In order to achieve a higher accuracy when testing the SVM classfier, I will try to extract as many numerical features as possible from the candidates and jobs dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the candidates and jobs dataframes with more columns to infere numerical features later on\n",
    "svm_candidates = candidates\n",
    "svm_jobs = jobs\n",
    "\n",
    "svm_candidates[\"Contract length\"] = np.nan\n",
    "svm_candidates[\"Expected Salary\"] = np.nan\n",
    "svm_candidates[\"Location\"] = np.nan\n",
    "\n",
    "svm_jobs[\"Contract length\"] = np.nan\n",
    "svm_jobs[\"Salary\"] = np.nan\n",
    "svm_jobs[\"Location\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random uk towns \n",
    "uk_locations = [\"London\",\"Edinburgh\",\"Manchester\",\"Bristol\",\"Birmingham\",\"Glasgow\",\"Liverpool\",\"Cardiff\",\"Belfast\",\"Newcastle\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b16b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silence pandas warning message\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4424b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add candidate's visa status, salary expectation, location (city), wanted contract length (3, 6, 9, 12 months)\n",
    "for i in range(len(candidates)):\n",
    "    svm_candidates.at[i, \"Contract length\"] = r.choice([3,6,9,12])\n",
    "    svm_candidates.at[i, \"Expected Salary\"] = r.choice([15000, 17500, 20000, 22500, 25000, 27500, 30000])\n",
    "    svm_candidates.at[i, \"Location\"] = r.choice(uk_locations)\n",
    "\n",
    "# add job's salary, contract length (3, 6, 9, 12 months), location (city)\n",
    "for i in range(len(jobs)):\n",
    "    svm_jobs.at[i, \"Contract length\"] = r.choice([3,6,9,12]) \n",
    "    svm_jobs.at[i, \"Salary\"] = r.choice([15000, 17500, 20000, 22500, 25000, 27500, 30000])\n",
    "    svm_jobs.at[i, \"Location\"] = r.choice(uk_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkout the new svm_candidates dataframe after population\n",
    "try:\n",
    "    svm_candidates.drop(\"Salary\", axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "svm_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkout the new svm_jobs dataframe after population\n",
    "svm_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf104f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the functions used to infere numerical features in the cell below\n",
    "def compute_field_xp_relevance(candidate_experience, job_experience):\n",
    "    return 1 if candidate_experience == job_experience else 0\n",
    "\n",
    "def compute_contract_length_compatibility(candidateID, jobID):\n",
    "    diff = svm_candidates.iloc[candidateID][\"Contract length\"] - svm_jobs.iloc[jobID][\"Contract length\"]\n",
    "    return diff//3\n",
    "\n",
    "def compute_location_compatibility(candidateID, jobID):\n",
    "    # return a random number between 0 and 500km for now, \n",
    "    # a future improvement will be to calculate the distance between the candidate's city and the job's city\n",
    "    return r.randint(0, 500)\n",
    "\n",
    "def compute_visa_requirements(candidateID, jobID):\n",
    "    distance = compute_location_compatibility(candidateID, jobID)\n",
    "    # if candidate is applying for a job 400+ km away they are more likely to be outside the UK\n",
    "    return 1 if distance > 400 else 0 \n",
    "\n",
    "def compute_offer_outcome(gpa_diff, field_xp_relevance, contract_compatibility, location_compatibility, visa_required, salary_diff):\n",
    "    offer_score = 0\n",
    "    \n",
    "    offer_score += 1 if gpa_diff < 20 else 0\n",
    "    offer_score += contract_compatibility\n",
    "    offer_score += 1 if location_compatibility < 400 else 0\n",
    "    offer_score += 0 if visa_required else 1\n",
    "    offer_score += 1 if salary_diff < 4000 else 0\n",
    "    \n",
    "    return 1 if offer_score >= 3.0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be921079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m svm_df_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# populate the table with 500 offers alongside their outcomes \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mcandidates\u001b[49m)):\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# get random candidate and jobs entries and their respective IDs\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     candidateID, jobID \u001b[38;5;241m=\u001b[39m i, random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(jobs)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m     candidate, job \u001b[38;5;241m=\u001b[39m svm_candidates\u001b[38;5;241m.\u001b[39miloc[candidateID], svm_jobs\u001b[38;5;241m.\u001b[39miloc[jobID]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'candidates' is not defined"
     ]
    }
   ],
   "source": [
    "svm_df_data = []\n",
    "\n",
    "# populate the table with 500 offers alongside their outcomes \n",
    "for i in range(0, len(candidates)):\n",
    "\n",
    "    # get random candidate and jobs entries and their respective IDs\n",
    "    candidateID, jobID = i, random.randint(0, len(jobs)-1)\n",
    "    candidate, job = svm_candidates.iloc[candidateID], svm_jobs.iloc[jobID]\n",
    "    \n",
    "    # extract the candidate gpa and job's minimum gpa required\n",
    "    candidate_gpa, job_gpa = svm_candidates.iloc[candidateID][\"Score\"], svm_jobs.iloc[jobID][\"MinScore\"]\n",
    "\n",
    "    # compute the compatibility score between the two selected entities\n",
    "    alpha_score = compute_compatibility(candidate, job)\n",
    "    \n",
    "    gpa_diff = candidate_gpa - job_gpa\n",
    "    field_xp_relevance = compute_field_xp_relevance(svm_candidates.iloc[candidateID][\"Experience\"], svm_jobs.iloc[jobID][\"Field\"])\n",
    "    contract_compatibility = compute_contract_length_compatibility(candidateID, jobID)\n",
    "    location_compatibility = compute_location_compatibility(candidateID, jobID)\n",
    "    visa_required = compute_visa_requirements(candidateID, jobID)\n",
    "    salary_diff = svm_jobs.iloc[jobID][\"Salary\"] - svm_candidates.iloc[candidateID][\"Expected Salary\"]\n",
    "    \n",
    "    offer_outcome = compute_offer_outcome(\n",
    "        gpa_diff, field_xp_relevance, contract_compatibility, location_compatibility, visa_required, salary_diff\n",
    "    )\n",
    "    \n",
    "    row = [ candidateID, jobID, candidate_gpa, job_gpa, alpha_score, gpa_diff, field_xp_relevance, contract_compatibility, location_compatibility, visa_required,salary_diff, offer_outcome, ]\n",
    "    svm_df_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_df_cols = [\n",
    "    'candidate ID', \n",
    "    'job ID', \n",
    "    'candidate GPA', \n",
    "    'job min GPA', \n",
    "    'compatibility', \n",
    "    'gpa_diff', \n",
    "    'field-xp relevance', # relevance score for the candidate's experience vs the job's field\n",
    "    'contract length compatibility', # candidate's preferred contract length vs job contract length\n",
    "    'location compatibility', # distance from candidate's location vs location of the job\n",
    "    'visa sponsorship required', # yes/no to whether candidate requires visa sponsorship for the job\n",
    "    'salary expectation-paid diff', # difference of salary expectation for candidate vs job salary\n",
    "    'accepted offer'\n",
    "]\n",
    "\n",
    "svm_df = pd.DataFrame(data=svm_df_data, columns=svm_df_cols)\n",
    "svm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e25ce8",
   "metadata": {},
   "source": [
    "<a id=\"svm-implementation\"></a> <br>\n",
    "### Implementing the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = svm_df.drop(['candidate ID', 'job ID'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['accepted offer'], axis=1)\n",
    "y = df['accepted offer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1206bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a939cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the confusion matrix and classfication report for this model\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b5f54",
   "metadata": {},
   "source": [
    "<a id=\"svm-improvement\"></a> <br>\n",
    "### Improving the Precision of the SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e346aa",
   "metadata": {},
   "source": [
    "In this subsection, we carry out a grid search to find the combination of hyper-parameters passed into the above SVC classifier which yield the best output.\n",
    "\n",
    "- **Kernel** : The main function of the kernel is to transform the dataset input data into a required form by extrapolating information from existing data and then lead to better classification. More in the [**evaluation**](#svm-evaluation) section.\n",
    "\n",
    "- **Regularization** : Regularization is implemented via the *C* parameter in the model, and it is used by the SVC as a guide on how much misclassification error is tolerated and controls the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n",
    "\n",
    "- **Gamma** : A lower value of gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting. In other words, a low value of gamma considers only nearby points in calculating the separation line, while a higher value of gamma considers all the data points in the calculation of the separation line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec981c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter values range for the SVC classifier\n",
    "param_grid = {\n",
    "    'C' : [0.1, 1, 10, 100, 1000, 10000, 100000],\n",
    "    'gamma' : [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a GridSearch to find the best combination of C and gamma values for the SVC classifier\n",
    "grid = GridSearchCV(SVC(), param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f922a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84157e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c09949",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b94501",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68430e",
   "metadata": {},
   "source": [
    "<a id=\"svm-evaluation\"></a> <br>\n",
    "### Evaluation of the SVM classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166b576",
   "metadata": {},
   "source": [
    "In conclusion, the SVM classifier trained in this section yielded an impressive 92% precision score when classifying offers as \"likely to be accepted\" or \"likely to be rejected\". This score has been achieved by first carrying out a SVM classification with default parameters and no kernel, and then improved by experimenting with different kernel functions and C-gamma hyper-parameter combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e917ce7",
   "metadata": {},
   "source": [
    "**Noticeable observations** include:\n",
    "\n",
    "1. Initially the model has been trained on a dataframe with 3 numerical columns which yielded a precision of 31%, but when training the model on 8 numerical columns a score of 65% was obtained. \n",
    "2. Without setting a kernel function in the SVC() instance but carrying out a grid-search with the C-gamma hyperparameters, the previous percentage reached values of 83%. \n",
    "3. Finally, experimenting with different kernel functions allowed the model to reach a precision of 92%, obtained through a Linear kernel function. Interestingly, using the Sigmoid or Polynomial kernel function lead to a significant decrease in the precision percentage of around 35-40%.\n",
    "4. Experimenting with different training/test splits of the input dataframe, 85/15 seemed to yield the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9edf9a",
   "metadata": {},
   "source": [
    "While the outputs of the confusion matrix and classification reports are very promising, I believe more training needs to carried out to deploy this model for real-world classification. The two main issues I identified where : \n",
    "\n",
    "1. Size of the input training dataset - the limited size of the input set (1000 candidates and 700-800 jobs) means that the model is trained on a very specific set of values for each feature, which might lead to an under-performing (overfitted) model in a real-world scenario. \n",
    "2. Quality of the input training dataset - since the original *candidates* and *jobs* dataframes contain randomly generated data, there is minimal correlation between datapoints so it is more challenging to do inference on the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95dafd",
   "metadata": {},
   "source": [
    "# KNN - work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of dataframe for computational performance reasons\n",
    "knn_df = df.drop(['gpa_diff', 'field-xp relevance', 'visa sponsorship required'], axis=1)[:250]\n",
    "\n",
    "grid = sns.PairGrid(knn_df)\n",
    "grid.map(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c115937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1db107",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(knn_df.drop('accepted offer', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f784a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_offers = scaler.transform(knn_df.drop('accepted offer', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55524c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7882640",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_df_feat = pd.DataFrame(acc_offers, columns=knn_df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91471437",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = knn_df_feat\n",
    "y = knn_df['accepted offer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fec7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow method\n",
    "error_rate = []\n",
    "\n",
    "for i in range(1, 100):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions_i = knn.predict(X_test)\n",
    "    # average error rate, where predicitons where not equal to actual test values \n",
    "    error_rate.append(np.mean(predictions_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeba2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.title('Error Rate vs K-Value')\n",
    "plt.xlabel('Number of Centroids (K)')\n",
    "plt.ylabel('Error Rate')\n",
    "\n",
    "plt.plot(range(1, 100), error_rate, color='black', marker='o', markerfacecolor='red', markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract k such that error is min\n",
    "k = error_rate.index(min(error_rate))\n",
    "print(f\"min error occurs at {k} centroids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d91627",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9520ccdd",
   "metadata": {},
   "source": [
    "# Logistic Regression - work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eafede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of dataframe for computational performance reasons\n",
    "logreg_df = df.drop(['gpa_diff', 'field-xp relevance', 'visa sponsorship required'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ce303",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = logreg_df.drop(['accepted offer'], axis=1)\n",
    "y = logreg_df['accepted offer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512812a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ef685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e2c0c",
   "metadata": {},
   "source": [
    "# Random Forest - work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['accepted offer'], axis=1)\n",
    "y = df['accepted offer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2061671",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a single decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting decision tree\n",
    "\n",
    "columns=list(X_train.columns)\n",
    "dot_data = tree.export_graphviz(dtree,out_file=None,feature_names=columns,class_names=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"image\",view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07887755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f830b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b953a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f451803",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098632e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c409a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194f36a",
   "metadata": {},
   "source": [
    "<a id=\"project-conclusion\"></a> <br>\n",
    "# Project Conclusion\n",
    "[back to top](#table-of-contents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
